# -*- coding: utf-8 -*-
"""POS_in_NLP_using_Pytorch_and_BiLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQNlDLVl70g1ozCCHMR_1VewLlQ5u1vE
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, LSTM, Dense,Flatten
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
data = pd.read_csv('/content/Kikuyu_Words.csv')  # Assuming the dataset is stored in 'french_pos_data.csv'

data.head(81)

# Convert all ũ and ĩ characters to u and i
data["Word"] = data["Word"].str.replace("ũ", "u")
data["Word"] = data["Word"].str.replace("ĩ", "i")

# Check the updated data
print(data["Word"])

data['Word'] = data['Word'].str.lower()

# Initialize the OneHotEncoder object
encoder = OneHotEncoder()

# Preprocessing
X = data['Word'].values
# y = pd.get_dummies(data['Label']).values

# Print values of X
X

# Fit the encoder on the target values
# Convert the encoded target values to a 2D array
y = data['Label'].values.reshape(-1, 1)

# Fit the encoder on the target values
encoder.fit(y)

# Transform the target values
y = encoder.transform(y)

# Convert the encoded target values to a 2D array
y = y.toarray()

# Print the encoded target values
print(y)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize words and pad sequences
word_to_index = {word: idx + 1 for idx, word in enumerate(set(X))}
X_train_tokens = np.array([[word_to_index[word] for word in sentence.split()] for sentence in X_train])
X_test_tokens = np.array([[word_to_index[word] for word in sentence.split()] for sentence in X_test])

max_sequence_length = max(max(len(x) for x in X_train_tokens), max(len(x) for x in X_test_tokens))
X_train_padded = pad_sequences(X_train_tokens, maxlen=max_sequence_length, padding='post')
X_test_padded = pad_sequences(X_test_tokens, maxlen=max_sequence_length, padding='post')

# Define and train the BiLSTM model
model = Sequential()
model.add(Embedding(input_dim=len(word_to_index) + 1, output_dim=100, input_length=max_sequence_length))
model.add(Flatten())
model.add(Bidirectional(LSTM(128, return_sequences=True)))
# model.add(Dense(len(data['Label'].unique()), activation='softmax'))
model.add(Dense(len(data['Label'].unique()), activation='softmax'))

# model summary
model.summary()

# Defining loss function and the appropriate optimizer
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Training the model
model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_test_padded, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')